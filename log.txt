V1.0-V3.0

03/20/21: Beginning of project, primarily addressed loading text and began building model, though not very successful results yet. First model yielded mediocre results from an approximate view, but could not predict, so attempted model 2, with a for loop over single LSTM and dense layer for each time step, but model compilation unsuccessful (fixing bugs also took a long time, particularly be careful of output dimensions). So I developed model 3, which used same LSTM structure as model 1, but more layers (also reorganized data generation during model 2 development) so a pretty clean and slightly better results than previous models. Also attempted MinMaxScaler but was unsuccessful. Even though model 3 had decent results, it could not predict due to the nature of its architecture (a many-to-many model).

V4.0-V4.1

03/21/21: Developed model 4. Model 4 uses a different architecture, using T_x timesteps to project the value n_lag days in the future. Started off with a simple model and developed deeper model. Same as before, started off disorganized with the dataset generation because it is temporary.

03/22/21: Included pandas library for data import. Significant difficulty at first upscaling from quarterly to daily, attempting a pivot table, but failed to assign values properly (always returned NaN), same with a for loop over groups. Lambda with groups yielded desired results.

03/23/21: Success with merging two datasets by Ticker, Date and exporting to numpy. Further success with implementing YoY revenue growth and cleaning data to remove any incomplete data. Began building model, but returned NaN loss value.

03/24/21: Simplification and cleaning of code. Transformed data using MinMaxScaler. Model now able to run successfully. Need to fix naming. Further simplification of code into methods. Improved model with additional layers, improved scaling, and created rudimentary prediction graph. Pivot table changed column order to alphabetical so reordered in order to be compatible with MinMaxScaler.

03/25/21: Implemented mean squared error loss function with custom weighting. Weighting: weights_m1 [100,1,1,1,10]. Debugged predict ability and potential model issues including MAPE loss, using logs. Tested on Tx=1, n_forward=1. Some of the variance in the results came from random initialization. MAPE seems to underperform MSE in most cases, especially with a log preprocess. Recompiling/reinitializing model is critical. Implemented prediction in procedural format. Training error in high digits of 1e-6. Experimenting with different T_x and n_forward values.

03/26/21: Tx360 n_f360 batches15 results are not strong because prediction cost does not have a strong effect on some of the output units, and those units are necessary for strong predictions (maybe not actually, since it is only looking at true values when n_predictions < n_forward). May consider cutting from 5 inputs to 2 inputs to see if improves situation. UAL Tx360 n_f360 batches15 weight only on 1st output unit -> decent output, but will need to look at other stocks. UAL Tx360 n_f360 batches15 weight uniform on all output units -> worse output results. AAPL Tx360 n_f360 batches15 loss weight uniform -> flat prediction for the most part. May need to double check prediction process. Same as previous but 2x loss weight on first unit -> better prediction. Perhaps a bit of overfitting, but that will improve with expanding to entire dataset. Implemented coloring to prediction graph for better visualization. Attempted related model (v5.1.1) with dense neural network to take one input and project forward many years, decent, but not super precise results. Prior to v5.1.1, tried LSTM with Tx1, and it seems LSTM performs worse when there are few timesteps. Cleaned prediction code into simplified methods. Cleaned code for inverse scaling for plotting, debugged custom loss functions. All Tx5 n_f360 batches5 yielded pretty strong results. Lessons learned: LSTMs perform poorly with few timesteps, MinMaxScale then log then MinMaxScale works pretty well, weighted 2x for loss on first unit seems to provide strong results, mean squared error works.

03/27/21: Tx200/7 n_f360/7 batches8 2x loss weight on unit 1 results in some kind of up and down sequence with little predictive power and pretty bad results. Tx5 n_f360 batches8 2x loss weight on unit 1 pretty good results. Attempting to fix the issue of MAPE loss metric being extremely large. I think part of the reason was that the second MinMaxScaler had a low end of 1e-16, so the loss would not be able to appropriately account for many of those because it would just be so small that even a tiny change of 1e-10 or something like that would lead to a massive MAPE increase. Two possibilities are to have a range that starts at a little above 0, or to use a different system of standardization. The latter addresses another issue, namely that the MinMaxScaler has predefined min and max, and if there are outliers, it can significantly impact the loss calculation. Tx5 n_f360 batches8 with standard scaler decent results. Will try feature normalization through divide by mean and L2 norm. Moving away from MinMax.


Done:
- Filter only mid-cap and above stocks
- Implement effective prediction and visualization
- Implement one forward prediction
- Option for weekly instead of daily

Still left to do: